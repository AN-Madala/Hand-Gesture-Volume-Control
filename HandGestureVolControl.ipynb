{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931aa845-56da-42aa-8ac4-739121ecec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # 0 means default webcam\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    \n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"Webcam\", frame)\n",
    "# Press 'spacebar' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe20fa82-76c6-4865-a669-a330320332f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10.21\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "print(mp.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7f5cd7d-4c8a-4487-ae27-57ebc465f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Create Hands object\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Process the frame and detect hands\n",
    "    result = hands.process(rgb_frame)\n",
    "\n",
    "    # Draw hand landmarks\n",
    "    if result.multi_hand_landmarks:\n",
    "        for hand_landmarks in result.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                hand_landmarks,\n",
    "                mp_hands.HAND_CONNECTIONS\n",
    "            )\n",
    "    cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcf0ef99-33b2-462d-a3e4-1a6a8641932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "#Pycaw Setup\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "volRange = volume.GetVolumeRange() \n",
    "minVol = volRange[0]\n",
    "maxVol = volRange[1]\n",
    "\n",
    "#MediaPipe Setup\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            lmList = []\n",
    "            for id, lm in enumerate(handLms.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy])\n",
    "            \n",
    "            if lmList:\n",
    "                x1, y1 = lmList[4][1], lmList[4][2]\n",
    "                x2, y2 = lmList[8][1], lmList[8][2] \n",
    "                cv2.circle(img, (x1, y1), 10, (255, 0, 255), cv2.FILLED)\n",
    "                cv2.circle(img, (x2, y2), 10, (255, 0, 255), cv2.FILLED)\n",
    "                cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "                \n",
    "                # Calculate distance using Eucleadean Distance\n",
    "                length = math.hypot(x2 - x1, y2 - y1)\n",
    "                \n",
    "                # Map length (50-300 pixels) to Volume (-65 to 0)\n",
    "                vol = np.interp(length, [50, 250], [minVol, maxVol])\n",
    "                volume.SetMasterVolumeLevel(vol, None)\n",
    "                \n",
    "              \n",
    "\n",
    "    cv2.imshow(\"Volume Control\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea323fd7-e3e8-4ef2-90e3-46e4446ad25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Volume Bar to previous Code\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import math\n",
    "import numpy as np\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "volRange = volume.GetVolumeRange()\n",
    "minVol = volRange[0]\n",
    "maxVol = volRange[1]\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(img_rgb)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for handLms in results.multi_hand_landmarks:\n",
    "            lmList = []\n",
    "            for id, lm in enumerate(handLms.landmark):\n",
    "                h, w, c = img.shape\n",
    "                cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                lmList.append([id, cx, cy])\n",
    "            \n",
    "            if lmList:\n",
    "                x1, y1 = lmList[4][1], lmList[4][2]\n",
    "                x2, y2 = lmList[8][1], lmList[8][2]\n",
    "                cv2.circle(img, (x1, y1), 10, (255, 0, 255), cv2.FILLED)\n",
    "                cv2.circle(img, (x2, y2), 10, (255, 0, 255), cv2.FILLED)\n",
    "                cv2.line(img, (x1, y1), (x2, y2), (255, 0, 255), 3)\n",
    "                length = math.hypot(x2 - x1, y2 - y1)\n",
    "                vol = np.interp(length, [50, 250], [minVol, maxVol])\n",
    "                volume.SetMasterVolumeLevel(vol, None)\n",
    "                \n",
    "                # Visual Volume Bar\n",
    "                volBar = np.interp(length, [50, 250], [400, 150])\n",
    "                cv2.rectangle(img, (50, 150), (85, 400), (0, 255, 0), 3)\n",
    "                cv2.rectangle(img, (50, int(volBar)), (85, 400), (0, 255, 0), cv2.FILLED)\n",
    "\n",
    "    cv2.imshow(\"Volume Control\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6b53cba-2c74-4b47-956c-8498a9a6204d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m lm \u001b[38;5;241m=\u001b[39m hand\u001b[38;5;241m.\u001b[39mlandmark\n\u001b[0;32m     25\u001b[0m xb, yb \u001b[38;5;241m=\u001b[39m lm[\u001b[38;5;241m9\u001b[39m]\u001b[38;5;241m.\u001b[39mx, lm[\u001b[38;5;241m9\u001b[39m]\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m---> 26\u001b[0m hand_scale \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mhypot(xb \u001b[38;5;241m-\u001b[39m \u001b[43mxw\u001b[49m, yb \u001b[38;5;241m-\u001b[39m yw)\n\u001b[0;32m     27\u001b[0m x1, y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(lm[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m*\u001b[39m w), \u001b[38;5;28mint\u001b[39m(lm[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m h)\n\u001b[0;32m     28\u001b[0m x2, y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(lm[\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m*\u001b[39m w), \u001b[38;5;28mint\u001b[39m(lm[\u001b[38;5;241m8\u001b[39m]\u001b[38;5;241m.\u001b[39my \u001b[38;5;241m*\u001b[39m h)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'xw' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "import math\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, model_complexity=1)\n",
    "draw_utils = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success: break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = hands.process(rgb_frame)\n",
    "\n",
    "    if output.multi_hand_landmarks:\n",
    "        for hand in output.multi_hand_landmarks:\n",
    "            lm = hand.landmark\n",
    "\n",
    "            xb, yb = lm[9].x, lm[9].y\n",
    "            hand_scale = math.hypot(xb - xw, yb - yw)\n",
    "            x1, y1 = int(lm[4].x * w), int(lm[4].y * h)\n",
    "            x2, y2 = int(lm[8].x * w), int(lm[8].y * h)\n",
    "            finger_gap = math.hypot(lm[8].x - lm[4].x, lm[8].y - lm[4].y)\n",
    "\n",
    "            gap_ratio = finger_gap / (hand_scale + 0.001)\n",
    "            fingers_open = []\n",
    "            for tip, pip in zip([8, 12, 16, 20], [6, 10, 14, 18]):\n",
    "                fingers_open.append(lm[tip].y < lm[pip].y) \n",
    "            if all(f == False for f in fingers_open):\n",
    "                pyautogui.press(\"volumemute\")\n",
    "                cv2.putText(frame, \"MUTED\", (w//2-50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 3)\n",
    "                cv2.waitKey(500)\n",
    "            elif all(f == True for f in fingers_open):\n",
    "                cv2.putText(frame, \"VOLUME LOCKED! EXITING...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 3)\n",
    "                cv2.imshow(\"Hand Volume Control\", frame)\n",
    "                cv2.waitKey(2000) \n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "            else:\n",
    "                vol_level = np.interp(gap_ratio, [0.3, 1.2], [0, 100])\n",
    "                pyautogui.run(f\"setvolume {int(vol_level)}\") \n",
    "                if gap_ratio > 0.9: pyautogui.press(\"volumeup\")\n",
    "                elif gap_ratio < 0.4: pyautogui.press(\"volumedown\")\n",
    "                    \n",
    "                cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "                cv2.putText(frame, f\"Vol Level: {int(vol_level)}%\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2)\n",
    "\n",
    "            draw_utils.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"Hand Volume Control\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '): break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7347d3e2-f0e2-4518-bfb3-027653a30c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "import math\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, model_complexity=0)\n",
    "draw_utils = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success: break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    h, w, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = hands.process(rgb_frame)\n",
    "\n",
    "    if output.multi_hand_landmarks:\n",
    "        for hand in output.multi_hand_landmarks:\n",
    "            lm = hand.landmark\n",
    "\n",
    "            xw, yw = lm[0].x, lm[0].y  \n",
    "            hand_scale = math.hypot(xb - xw, yb - yw)\n",
    "\n",
    "            x1, y1 = int(lm[4].x * w), int(lm[4].y * h)\n",
    "            x2, y2 = int(lm[8].x * w), int(lm[8].y * h)\n",
    "\n",
    "            finger_gap = math.hypot(lm[8].x - lm[4].x, lm[8].y - lm[4].y)\n",
    "            gap_ratio = finger_gap / (hand_scale + 0.001)\n",
    "\n",
    "            fingers_open = []\n",
    "            for tip, pip in zip([8, 12, 16, 20], [6, 10, 14, 18]):\n",
    "                fingers_open.append(lm[tip].y < lm[pip].y) )\n",
    "            if not any(fingers_open):\n",
    "                pyautogui.press(\"volumemute\")\n",
    "                cv2.putText(frame, \"MUTED\", (w//2-50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 3)\n",
    "                cv2.waitKey(500) \n",
    "            elif all(fingers_open): \n",
    "                cv2.putText(frame, \"EXITING...\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255,0), 3)\n",
    "                cv2.imshow(\"Hand Volume Control\", frame)\n",
    "                cv2.waitKey(1000) \n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit())\n",
    "            else:\n",
    "                vol_level = np.interp(gap_ratio, [0.2, 1.0], [0, 100])\n",
    "                \n",
    "                if gap_ratio > 0.8: \n",
    "                    pyautogui.press(\"volumeup\")\n",
    "                elif gap_ratio < 0.3: \n",
    "                    pyautogui.press(\"volumedown\")\n",
    "                    \n",
    "                cv2.line(frame, (x1, y1), (x2, y2), (255, 0, 0), 3)\n",
    "                cv2.putText(frame, f\"Vol: {int(vol_level)}%\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 2)\n",
    "\n",
    "            draw_utils.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"Hand Volume Control\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '): break \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23008632-b776-45d8-81d0-07f34c00f344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import math\n",
    "from ctypes import cast, POINTER\n",
    "from comtypes import CLSCTX_ALL\n",
    "from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume\n",
    "\n",
    "# Initialize Pycaw\n",
    "devices = AudioUtilities.GetSpeakers()\n",
    "interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)\n",
    "volume = cast(interface, POINTER(IAudioEndpointVolume))\n",
    "\n",
    "# Initialize MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.8, model_complexity=1)\n",
    "draw_utils = mp.solutions.drawing_utils\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success: break\n",
    "    \n",
    "    frame = cv2.flip(frame, 1) \n",
    "    h, w, _ = frame.shape\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    output = hands.process(rgb_frame)\n",
    "\n",
    "    if output.multi_hand_landmarks:\n",
    "        for hand in output.multi_hand_landmarks:\n",
    "            lm = hand.landmark\n",
    "            palmW = math.hypot(lm[5].x - lm[0].x, lm[5].y - lm[0].y)\n",
    "            finger_gap = math.hypot(lm[8].x - lm[4].x, lm[8].y - lm[4].y)\n",
    "            ratio = finger_gap / (palmW + 0.001)\n",
    "\n",
    "        \n",
    "            tips = [lm[8].y, lm[12].y, lm[16].y, lm[20].y]\n",
    "            joints = [lm[6].y, lm[10].y, lm[14].y, lm[18].y]\n",
    "            fingers_open = [tips[i] < joints[i] for i in range(4)]\n",
    "\n",
    "            if not any(fingers_open):\n",
    "                volume.SetMute(1, None)\n",
    "                status = \"MUTED\"\n",
    "                color = (0, 0, 255)\n",
    "\n",
    "            elif all(fingers_open):\n",
    "                status = \"LOCKED & EXITING\"\n",
    "                cv2.putText(frame, status, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                cv2.imshow(\"Volume Control\", frame)\n",
    "                cv2.waitKey(1000)\n",
    "                cap.release()\n",
    "                cv2.destroyAllWindows()\n",
    "                exit()\n",
    "            \n",
    "            else:\n",
    "                volume.SetMute(0, None)\n",
    "                vol_level = np.interp(ratio, [0.2, 1.1], [0.0, 1.0])\n",
    "                volume.SetMasterVolumeLevelScalar(vol_level, None)\n",
    "                \n",
    "                status = f\"Volume: {int(vol_level * 100)}%\"\n",
    "                color = (255, 255, 0)\n",
    "                \n",
    "            cv2.putText(frame, status, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "            draw_utils.draw_landmarks(frame, hand, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    cv2.imshow(\"Volume Control\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(' '): break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8116dfb-43c3-49fe-867f-e99bb8436892",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(mediapipe)",
   "language": "python",
   "name": "mediapipe_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
